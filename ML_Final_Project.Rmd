---
title: "Analyzing Weightlifting through Wearables"
author: "Scott Knapp"
date: "8/20/2020"
output:
        html_document:
                toc: true
                toc_float: true
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(fig.width = 8, fig.height = 4,
                      warning=FALSE, message=FALSE)
```

## Executive Summary

As part of Course 7, Practical Machine Learning in the Data Science Specialization on Coursera, the goal of this project is to predict the manner in which participants did an exercise.  This is the 'classe' variable in the below dataset.  We will clean and explore the data to choose the variables to predict with.  This report will outline how the models are built, estimate our out-of-sample error utilizing validation tests, and explain how we choose the best model to utilize.  Lastly, we will use this model to predict 20 test cases.

## Background

This human activity recognition research has traditionally focused on discriminating between different activities, i.e. to predict "which" activity was performed at a specific point in time (like with the Daily Living Activities dataset above). The approach we propose for the Weight Lifting Exercises dataset is to investigate "how (well)" an activity was performed by the wearer. The "how (well)" investigation has only received little attention so far, even though it potentially provides useful information for a large variety of applications,such as sports training.

In this work (see the paper referenced) we first define quality of execution and investigate three aspects that pertain to qualitative activity recognition: the problem of specifying correct execution, the automatic and robust detection of execution mistakes, and how to provide feedback on the quality of execution to the user. We tried out an on-body sensing approach (dataset here), but also an "ambient sensing approach" (by using Microsoft Kinect - dataset still unavailable)

Six young health participants were asked to perform one set of 10 repetitions of the Unilateral Dumbbell Biceps Curl in five different fashions: exactly according to the specification (Class A), throwing the elbows to the front (Class B), lifting the dumbbell only halfway (Class C), lowering the dumbbell only halfway (Class D) and throwing the hips to the front (Class E).


## Data Loading and Cleaning

### Setup and Libraries

```{r libraries}
library(caret)
library(dplyr)
```
### Loading the Data

```{r loading, cache = TRUE}
url_train <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
url_test <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"

train_raw <- read.csv(url_train)
test_raw <- read.csv(url_test)

dim(train_raw); dim(test_raw)
```

### Non-parametric Variables

```{r text_variables, results = "hide"}
View(train_raw)
```

After viewing the data it appears the first 6 features are not applicable to the 
analysis, so we can remove them.

```{r remove text_features}
train_data <- train_raw[,-c(1:6)]
test_data <- test_raw[, -c(1:6)]

dim(train_data); dim(test_data)
```
### Missing Values

We will need to address any NAs in the data.

```{r NAs}
table(sapply(train_data, function(x) mean(is.na(x))))
```

This is an interesting result, 87 features have no NA's, while 67 have 97%+ NA's.
Let's exclude these features for now.

```{r}
remove_NA <- sapply(train_data, function(x) mean(is.na(x))) == 0
table(remove_NA)

train_data <- train_data[, remove_NA == TRUE]
test_data <- test_data[, remove_NA == TRUE]
dim(train_data); dim(test_data)
```

### Zero-Variance Variables

Let's also remove the zero-variance features from the modeling.

```{r zero_var, cache = TRUE}
near_zero <- nearZeroVar(train_data)
train_data <- train_data[, -near_zero]
test_data <- test_data[, -near_zero]

dim(train_data); dim(test_data)
```

Lastly, let's convert Classe to a factor variable

```{r factors}
train_data$classe <- as.factor(train_data$classe)
```

## Partitioning the Datasets to Train and Validation

Now, with the data cleaned, we can create split our train_data set into 
training and validation sets to build our models against.

```{r partition}
inTrain <- createDataPartition(train_data$classe, p = 3/4, list = FALSE)
train_set <- train_data[inTrain, ]
valid_set <- train_data[-inTrain, ]

dim(train_set); dim(valid_set)
```

## Feature Engineering

This is a large dataset with many features, for speed optimization, let's perform
a Principal Components Analysis to see how many features will be needed to give us
the majority of the accuracy to improve model performance.

### PCA Analysis

```{r PCA}
train_set_num <- train_set %>%
        select(-classe) %>%
        mutate_all(as.numeric)

train_set_num[is.na(train_set_num)] = 0

pca <- prcomp(train_set_num)

qplot(1:length(pca$sdev), pca$sdev / sum(pca$sdev), ylab = "% Explained", 
      xlab = "# of Features")
```

From this chart it appears that we seem to get to 99% explaination in between 30 and 40
features. 

```{r PCA2}
cumsum(pca$sdev / sum(pca$sdev))[30:40]
```

Looks like 35 features will get us to 99% of the variance explained, so if we run into
performance issues, we can use this as a constraint on the models.

## Model Creation

Now it's time to build and train our various models.  Due to having limited computing
power on a single core, we will not utilze a Random Forest model.  We will instead train 2 models then tune the better performing model to improve results.

*  Recursive Tree
*  Gradient Boosted Machine using Cross Validation
*  Gradient Boosted Machine (user-tuned)

We will utilize a Confusion Matrix to compare the result on the Training Set.


### Recursive Tree (rpart)

```{r rpart, cache = TRUE}
set.seed(4242)

model_rpart <- train(classe ~ ., data = train_set, method = "rpart")

```

### Gradient Boosted Machine (gbm)

```{r gbm, cache = TRUE}
set.seed(4242)

train_control <- trainControl(method = "cv", number = 3)


model_gbm <- train(classe ~ ., data = train_set, method = "gbm", 
                   trControl = train_control, verbose = FALSE)


model_gbm$bestTune
```

### Gradient Boosted Machine User-Defined(gbm)

With more compute power and time, we can set the tuning grid to a range of 
values i.e.

- interaction.depth = c(3, 5, 7)
- n.trees = c(150, 175, 200, 225)
- shrinkage = c(0.075, 0.10, 0.125)
- n.minosinnode = c(7, 10, 12, 15)

Here, I have illustrated a single user-defined output.

```{r gbm_user, cache = TRUE}
set.seed(4242)

train_control <- trainControl(method = "cv", number = 3)
gbm_grid <- expand.grid(interaction.depth = 5,
                        n.trees = 175,
                        shrinkage = 0.075,
                        n.minobsinnode = 10)


model_gbm_user <- train(classe ~ ., data = train_set, method = "gbm", 
                   trControl = train_control, tuneGrid = gbm_grid, verbose = FALSE)

```

## Model Selection

Let's validate the model against the validation data

```{r validation, cache = TRUE}

predict_rpart <- predict(model_rpart, newdata = valid_set)
predict_gbm <- predict(model_gbm, newdata = valid_set)
predict_gbm_user <- predict(model_gbm_user, newdata = valid_set)

CM_rpart <- confusionMatrix(predict_rpart, valid_set$classe)
CM_gbm <- confusionMatrix(predict_gbm, valid_set$classe)
CM_gbm_user <- confusionMatrix(predict_gbm_user, valid_set$classe)

CM_rpart$table
CM_gbm$table
CM_gbm_user$table


CM_rpart$overall[1]; CM_gbm$overall[1]; CM_gbm_user$overall[1]
```


## Prediction

Now it's time to use our best model [model_gbm_user] to predict values for our Test set.  Given our success with the validation data, we would expect our out-of-sample
error to be small.

```{r prediction}
predict_rf <- predict(model_gbm_user, newdata = test_data)

predict_rf
```



## Acknowledgements

Velloso, E.; Bulling, A.; Gellersen, H.; Ugulino, W.; Fuks, H. 
*Qualitative Activity Recognition of Weight Lifting Exercises.* 
Proceedings of 4th International Conference in Cooperation with SIGCHI (Augmented Human '13) . Stuttgart, Germany: ACM SIGCHI, 2013.
